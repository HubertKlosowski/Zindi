{
 "cells": [
  {
   "cell_type": "code",
   "id": "fab525021207f8ad",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wczytanie danych",
   "id": "1bfd290735c3b2e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = pd.read_csv(os.path.join('data', 'train.csv'))\n",
    "test = pd.read_csv(os.path.join('data', 'test.csv'))\n",
    "# train_additional = pd.read_csv(os.path.join('data', 'train_layer.csv'))\n",
    "\n",
    "# train_additional.info()"
   ],
   "id": "ff47ed9db9745c15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.info()",
   "id": "ce58a6da1ddd875d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rozbicie daty na składowe",
   "id": "3a18f5123a735c7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def change_date(dataframe, sin_cos=True):\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "    dataframe['dayofweek'] = dataframe['date'].dt.dayofweek.astype(np.int64)\n",
    "    dataframe['week'] = dataframe['date'].dt.isocalendar().week.astype(np.int64)\n",
    "    dataframe['year'] = dataframe['date'].dt.year.astype(np.int64)\n",
    "    if 'month' not in dataframe.columns:\n",
    "        dataframe['month'] = dataframe['date'].dt.month.astype(np.int64)\n",
    "    # if sin_cos:\n",
    "    #     dataframe['sin_dayofweek'] = np.sin(2 * np.pi * dataframe['dayofweek'] / 7)\n",
    "    #     dataframe['cos_dayofweek'] = np.cos(2 * np.pi * dataframe['dayofweek'] / 7)\n",
    "    #     dataframe['sin_week'] = np.sin(2 * np.pi * dataframe['week'] / 52)\n",
    "    #     dataframe['cos_week'] = np.cos(2 * np.pi * dataframe['week'] / 52)\n",
    "    #     dataframe['sin_month'] = np.sin(2 * np.pi * dataframe['month'] / 12)\n",
    "    #     dataframe['cos_month'] = np.cos(2 * np.pi * dataframe['month'] / 12)\n",
    "        \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "train, test = change_date(train), change_date(test)\n",
    "# train = train[(train['date'] >= test['date'].min()) & (train['date'] <= test['date'].max())]\n",
    "# train.reset_index(drop=True, inplace=True)\n",
    "# train_additional = change_date(train_additional)"
   ],
   "id": "8d2c1f67577a7d8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wykres przedstawiający wartość pm2_5 w zarejestrowanych godzinach",
   "id": "7c788a9b35c1fab3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.barplot(data=train, x='hour', y='pm2_5')\n",
    "plt.title('Jakość powietrza w poszczególnych godzinach')"
   ],
   "id": "bf974b1f6654091f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wykres przedstawiający wartość pm2_5 w zależności od dnia tygodnia",
   "id": "f46748e403cbeb0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.barplot(data=train, x='dayofweek', y='pm2_5')\n",
    "plt.title('Jakość powietrza w każdym dniu tygodnia')"
   ],
   "id": "265fd4ba0d9ddfc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wykres przedstawiający wartość pm2_5 w zależności od miesiąca",
   "id": "e4107624bd3bca2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.barplot(data=train, x='month', y='pm2_5')\n",
    "plt.title('Jakość powietrza w każdym miesiącu')"
   ],
   "id": "6a91d2e567c5484e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Korelacja wybranych kolumn z pm2_5",
   "id": "13f1f13fb9cf45d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.heatmap(train[['month', 'week', 'dayofweek', 'year', 'hour', 'site_latitude', 'site_longitude', 'cloud_surface_albedo', 'pm2_5']].corr(), annot=True, cmap='Greys')",
   "id": "d38b36ac602de2ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wykres przedstawiający wartości pm2_5 w poszczególnych miastach",
   "id": "b1ae741b1f38f74d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=train, x='date', y='pm2_5', hue='city')\n",
    "plt.title('Jakość powietrza w kolejnych dniach')"
   ],
   "id": "82be217767d850b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wartości pm2_5 dla Lagos z podziałem na site_id",
   "id": "7711c20595ed298b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "only_lagos = train[train['city'] == 'Lagos'][['site_id', 'pm2_5']]\n",
    "\n",
    "only_lagos.plot(kind='hist', x='site_id')"
   ],
   "id": "902609d563e3bb36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Kolumna ozone_o3_effective_temperature",
   "id": "7d44cd37e06dd943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train['ozone_o3_effective_temperature'].describe()",
   "id": "1aa0e9c40dab5d5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ozone_o3_effective_temperature = train.loc[train['ozone_o3_effective_temperature'].isna(), ['city', 'week', 'dayofweek', 'month', 'pm2_5']].copy(deep=True)",
   "id": "8809e4b5b04f8e06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ozone_o3_effective_temperature.groupby('city')['month'].value_counts()",
   "id": "546456ca6493cec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.groupby(['city', 'month'])['ozone_o3_effective_temperature'].mean()",
   "id": "e2a0dada3f9334e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## <center>Czyszczenie danych</center>\n",
    "### 1. Imputacja, usuwanie kolumn, oraz inne cuda"
   ],
   "id": "15ed92155a919761"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "def fill_train(column_name='dayofweek'):\n",
    "    column_values = train[column_name].unique()\n",
    "    for date in column_values:\n",
    "        for i, column in enumerate(starts_with):\n",
    "            similar_columns = [col for col in train.columns if col.startswith(column)]\n",
    "            df = train.loc[train[column_name] == date, similar_columns].copy()\n",
    "            if not df.empty:\n",
    "                try:\n",
    "                    train.loc[train[column_name] == date, similar_columns] = imputers[i].fit_transform(df)\n",
    "                except ValueError:\n",
    "                    train.drop(index=df.index, inplace=True)\n",
    "                    train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def fill_test(column_name='dayofweek'):\n",
    "    column_values = test[column_name].unique()\n",
    "    for date in column_values:\n",
    "        for i, column in enumerate(starts_with):\n",
    "            similar_columns = [col for col in test.columns if col.startswith(column)]\n",
    "            df = test.loc[test[column_name] == date, similar_columns].copy()\n",
    "            if not df.empty:\n",
    "                test.loc[test[column_name] == date, similar_columns] = imputers[i].transform(df)\n",
    "\n",
    "def drop_high_nans():  # usuwamy kolumny o dużej liczbie wartości NaN\n",
    "    columns_nans = []\n",
    "    for i, el in enumerate(train.columns):\n",
    "        if train[el].isna().sum() / len(train) >= 0.9:\n",
    "            columns_nans.append(el)\n",
    "    return columns_nans\n",
    "\n",
    "def drop_high_correlated_columns():\n",
    "    matrix = train.corr(numeric_only=True).abs()\n",
    "    upper_t = matrix.where(np.triu(np.ones_like(matrix, dtype=np.bool_), k=1))\n",
    "    return [col for col in upper_t.columns if any(upper_t[col] > 0.99)]\n",
    "\n",
    "def drop_low_correlated_columns_to_pm2_5():\n",
    "    corr = train.corr()['pm2_5'].to_frame()\n",
    "    return corr[(corr['pm2_5'] < 0.01) & (corr['pm2_5'] > -0.01)].index.to_numpy()\n",
    "\n",
    "def subract_azimuth_zenith(dataframe):\n",
    "    zenith_columns = [zenith for zenith in dataframe.columns if 'zenith' in zenith]\n",
    "    azimuth_columns = [azimuth for azimuth in dataframe.columns if 'azimuth' in azimuth]\n",
    "    for i, zenith in enumerate(zenith_columns):\n",
    "        splitted = zenith.split('_')\n",
    "        dataframe[f'{splitted[0]}_{splitted[1]}_diff'] = dataframe[zenith] - dataframe[azimuth_columns[i]]\n",
    "        dataframe.drop(zenith_columns[i], axis=1, inplace=True)\n",
    "        dataframe.drop(azimuth_columns[i], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "starts_with = train.columns.str.split('_', expand=True).levels[0].to_frame()\n",
    "starts_with.drop(['month', 'pm2'], inplace=True)\n",
    "starts_with = starts_with[0].tolist()\n",
    "\n",
    "imputers = [KNNImputer(n_neighbors=15, weights='distance') for _ in range(len(starts_with))]\n",
    "high_nans = drop_high_nans()\n",
    "train, test = train.drop(columns=high_nans, axis=1), test.drop(columns=high_nans, axis=1)\n",
    "# fill_train(), fill_test()\n",
    "# to_drop = drop_low_correlated_columns_to_pm2_5()\n",
    "# to_drop = np.concatenate((to_drop, drop_high_correlated_columns()), axis=0)\n",
    "# train, test = train.drop(columns=to_drop, axis=1), test.drop(columns=to_drop, axis=1)\n",
    "# train, test = subract_azimuth_zenith(train), subract_azimuth_zenith(test)\n",
    "\n",
    "train.info()"
   ],
   "id": "4cf45820641da986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Połączenie danych z innych konkursów",
   "id": "80ffbbc967cc015b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train_additional.drop(columns=['ID', 'humidity', 'temp_mean', 'device', 'site_latitude', 'site_longitude', 'date'], inplace=True)\n",
    "# \n",
    "# for name in train_additional.columns.values:\n",
    "#     name_parts = name.split('_')\n",
    "#     name_parts_lower = [x.lower() for x in name_parts]\n",
    "#     train_additional.rename(columns={name: '_'.join(name_parts_lower)}, inplace=True)\n",
    "# \n",
    "# train = pd.concat([train, train_additional])\n",
    "# train.reset_index(drop=True, inplace=True)\n",
    "# \n",
    "# train.info()"
   ],
   "id": "81a9005ec2b73b6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wykresy pudełkowe wskazujące wartości odstające",
   "id": "f4dc17aac706bbd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sympy import divisors\n",
    "\n",
    "\n",
    "def plot_boxplots():\n",
    "    for i, column_group in enumerate(starts_with):\n",
    "        similar_columns = [col for col in train.columns if col.startswith(column_group)]\n",
    "        if len(similar_columns) > 1:\n",
    "            divs = divisors(len(similar_columns))\n",
    "            if len(divs) % 2 == 0:\n",
    "                rows, cols = divs[(len(divs) // 2) - 1], divs[len(divs) // 2]\n",
    "            else:\n",
    "                rows, cols = divs[len(divs) // 2], divs[len(divs) // 2]\n",
    "            fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(40, 30), squeeze=False)\n",
    "            fig.suptitle(column_group, fontsize=25)\n",
    "            for j, column in enumerate(similar_columns):\n",
    "                x_cord, y_cord = divmod(j, cols)\n",
    "                train[column].plot(kind='box', ax=ax[x_cord, y_cord], fontsize=15)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "vertical_columns = [col for col in train.columns if 'number_density' in col]\n",
    "\n",
    "# plot_boxplots()"
   ],
   "id": "67e9875241d501a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Wskazanie kwantyli, od których są outliery",
   "id": "3ac24b44e6bd3a85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "detect_outliers = zscore(train['pm2_5'])\n",
    "\n",
    "quantiles = pd.DataFrame(list(zip(np.linspace(0.95, 1, 20), [np.quantile(detect_outliers, el) for el in np.linspace(0.95, 1, 20)])), columns=['quantile', 'zscore'])\n",
    "quantiles"
   ],
   "id": "e7d3f816a2bb205d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Usunięcie wartości odstających",
   "id": "24c6669735dee8d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def del_pm2_5_outliers():\n",
    "    indexes_to_drop = []\n",
    "    q1, q2 = np.quantile(detect_outliers, 0.01), np.quantile(detect_outliers, 0.99)\n",
    "    for i, el in enumerate(detect_outliers):\n",
    "        if el > q2:\n",
    "            indexes_to_drop.append(i)\n",
    "    train.drop(indexes_to_drop, inplace=True)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "def scale_pm2_5_outliers():\n",
    "    out = train[zscore(train['pm2_5']) > np.quantile(detect_outliers, 0.99)]['pm2_5'].copy(deep=True)\n",
    "    out *= 0.6\n",
    "    train.loc[out.index, 'pm2_5'] = out\n",
    "\n",
    "\n",
    "# scale_pm2_5_outliers()\n",
    "del_pm2_5_outliers()\n",
    "\n",
    "train.info()"
   ],
   "id": "a3c1c6860a3b0de4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Usuwanie outlierów na podstawie rady marching_learning",
   "id": "da5a062992a03500"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "joined = pd.merge(only_lagos, detect_outliers, left_index=True, right_index=True)\n",
    "outliers_from_lagos = joined[joined['pm2_5_y'] > 3.0]['site_id'].value_counts()\n",
    "site_ids_to_del = outliers_from_lagos.keys()[:2]\n",
    "outliers_from_lagos"
   ],
   "id": "5f37ea49ec020e3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# indexes = train[train['site_id'].isin(site_ids_to_del)].index\n",
    "# train.drop(index=indexes, inplace=True)\n",
    "# train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# train.info()"
   ],
   "id": "c152f1e37847488a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## <center>Selekcja cech</center>",
   "id": "aa4a1dd941bba4fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import RFECV, RFE, SelectKBest, mutual_info_regression, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def plot_feature_importance(sc, num_of_features):\n",
    "    if isinstance(sc, RFECV) or isinstance(sc, RFE):\n",
    "        scores = dict(zip(sc.feature_names_in_, sc.ranking_))\n",
    "    else:\n",
    "        scores = dict(zip(sc.feature_names_in_, sc.scores_))\n",
    "    scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:num_of_features]\n",
    "    scores_df = pd.DataFrame(scores, columns=['Feature', 'Score'])\n",
    "\n",
    "    scores_df.plot(kind='bar', x='Feature', y='Score', figsize=(10, 6), rot=90, title='Oceny wybranych cech')\n",
    "    plt.xlabel('Cecha')\n",
    "    plt.ylabel('Ocena')\n",
    "\n",
    "\n",
    "# selector = RFE(\n",
    "#     estimator=RandomForestRegressor(\n",
    "#         n_estimators=700, \n",
    "#         max_depth=7, \n",
    "#         random_state=4, \n",
    "#         n_jobs=-1, \n",
    "#         oob_score=True,\n",
    "#         warm_start=True\n",
    "#     ),\n",
    "#     n_features_to_select=k,\n",
    "# )\n",
    "# k = 17\n",
    "# selector = RFECV(\n",
    "#     estimator=RandomForestRegressor(\n",
    "#         n_estimators=400, \n",
    "#         max_depth=10, \n",
    "#         random_state=4, \n",
    "#         n_jobs=-1, \n",
    "#         oob_score=True, \n",
    "#         warm_start=True, \n",
    "#         ccp_alpha=1e-4\n",
    "#     ),\n",
    "#     min_features_to_select=k, \n",
    "#     cv=10, \n",
    "#     scoring='neg_root_mean_squared_error',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# selector.fit(train, y)\n",
    "# train, test = selector.transform(train), selector.transform(test)\n",
    "# \n",
    "# plot_feature_importance(selector, k)"
   ],
   "id": "b5b7d04884cab66d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## <center>Transformacja danych</center>\n",
    "### 1. Standaryzacja danych"
   ],
   "id": "3cdd37ba5459c448"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, TargetEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "test_ids = test['id']\n",
    "train.drop(columns=['id', 'city', 'date', 'country', 'site_id', 'site_latitude', 'site_longitude', 'hour'], inplace=True)\n",
    "test.drop(columns=['id', 'city', 'date', 'country', 'site_id', 'site_latitude', 'site_longitude', 'hour'], inplace=True)\n",
    "\n",
    "train_pm2_5 = train['pm2_5']\n",
    "train.drop(columns=['pm2_5'], inplace=True)"
   ],
   "id": "b588a29d657f3df7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Z podziałem na kategoryczne",
   "id": "82aa10574502435d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical = ['month', 'dayofweek', 'week', 'year']\n",
    "train[categorical] = train[categorical].astype('category')\n",
    "test[categorical] = test[categorical].astype('category')\n",
    "\n",
    "non_cat_columns = train.columns.difference(categorical)\n",
    "\n",
    "non_cat_scaler = make_column_transformer((RobustScaler(), non_cat_columns))\n",
    "te = TargetEncoder(random_state=4)\n",
    "\n",
    "train_cat, test_cat = te.fit_transform(train[categorical], train_pm2_5), te.transform(test[categorical])\n",
    "\n",
    "train = pd.concat([pd.DataFrame(non_cat_scaler.fit_transform(train[non_cat_columns]), columns=non_cat_scaler.feature_names_in_), pd.DataFrame(train_cat, columns=te.feature_names_in_), train_pm2_5], axis=1)\n",
    "test = pd.concat([test_ids, pd.DataFrame(non_cat_scaler.transform(test[non_cat_columns]), columns=non_cat_scaler.feature_names_in_), pd.DataFrame(test_cat, columns=te.feature_names_in_)], axis=1)\n",
    "\n",
    "# train = train.round(decimals=3)"
   ],
   "id": "cf6c24f2f6db0a83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Wszystkie kolumny",
   "id": "2dd23a2050be5bc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# scaler = RobustScaler()\n",
    "# \n",
    "# train, test = scaler.fit_transform(train), scaler.transform(test)\n",
    "# train, test = pd.concat([pd.DataFrame(train, columns=scaler.feature_names_in_), train_pm2_5], axis=1), pd.concat([test_ids, pd.DataFrame(test, columns=scaler.feature_names_in_)], axis=1)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Zapis przekształconych danych",
   "id": "f39e5e9b5d6db6f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'transform' not in os.listdir(os.getcwd()):\n",
    "    os.mkdir('transform')\n",
    "train.to_csv(os.path.join('transform', 'train.csv'), index=False)\n",
    "test.to_csv(os.path.join('transform', 'test.csv'), index=False)"
   ],
   "id": "6fe27a6a613d3b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Po prostu xd",
   "id": "26cb836f54ab6642"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xd = pd.read_csv(os.path.join('data', 'xd.csv'), header=0)\n",
    "xd['pm2_5'] *= 1.03\n",
    "xd.to_csv(os.path.join('data', 'xd1.csv'), index=False)"
   ],
   "id": "27b4bead0508e55f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
